# üß† Guia Completo: HippocampAI para Chatbots
## Implementa√ß√£o de Mem√≥ria de Longo Prazo em Sistemas Conversacionais

---

## üìã Sum√°rio

1. [Vis√£o Geral](#1-vis√£o-geral)
2. [Fundamentos Te√≥ricos](#2-fundamentos-te√≥ricos)
3. [Arquitetura do HippocampAI](#3-arquitetura-do-hippocampai)
4. [Tipos de Mem√≥ria](#4-tipos-de-mem√≥ria)
5. [Instala√ß√£o e Configura√ß√£o](#5-instala√ß√£o-e-configura√ß√£o)
6. [Implementa√ß√£o no Chatbot](#6-implementa√ß√£o-no-chatbot)
7. [Casos de Uso](#7-casos-de-uso)
8. [Considera√ß√µes de Performance](#8-considera√ß√µes-de-performance)
9. [Troubleshooting](#9-troubleshooting)
10. [Refer√™ncias](#10-refer√™ncias)

---

## 1. Vis√£o Geral

### 1.1 O Problema: Chatbots Esquecem

Chatbots tradicionais sofrem de **amn√©sia contextual**:
- Perdem o contexto entre sess√µes
- N√£o lembram prefer√™ncias do usu√°rio
- Falham em conectar informa√ß√µes dispersas
- Repetem informa√ß√µes j√° fornecidas

### 1.2 A Solu√ß√£o: HippocampAI

O **HippocampAI** √© uma engine de mem√≥ria inspirada na neurobiologia do hipocampo humano. Ele adiciona:

- ‚úÖ **Mem√≥ria de longo prazo** persistente
- ‚úÖ **Knowledge Graph** para relacionamentos
- ‚úÖ **Recupera√ß√£o h√≠brida** (sem√¢ntica + lexical + grafo)
- ‚úÖ **Consolida√ß√£o de mem√≥ria** (fase de sono)
- ‚úÖ **Multi-agente** com mem√≥ria compartilhada

### 1.3 Diferen√ßa para RAG Tradicional

| Aspecto | RAG Tradicional | HippocampAI |
|---------|----------------|-------------|
| **Armazenamento** | Vetores densos | Vetores + Grafo de Conhecimento |
| **Recupera√ß√£o** | Similaridade sem√¢ntica | PageRank Personalizado + RRF |
| **Relacionamentos** | Impl√≠citos | Expl√≠citos (triplas) |
| **Atualiza√ß√£o** | Reindexa√ß√£o completa | Atualiza√ß√£o incremental |
| **Explicabilidade** | Baixa | Alta (caminhos de recupera√ß√£o) |

---

## 2. Fundamentos Te√≥ricos

### 2.1 Teoria do Indexamento Hipocampal

Baseado na neuroci√™ncia:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    C√âREBRO HUMANO                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Neoc√≥rtex (C√≥rtex Temporal)   ‚îÇ   Hipocampo               ‚îÇ
‚îÇ  ‚Ä¢ Armazena mem√≥rias           ‚îÇ   ‚Ä¢ √çndice din√¢mico       ‚îÇ
‚îÇ  ‚Ä¢ Representa√ß√£o distribu√≠da   ‚îÇ   ‚Ä¢ Ponteiros associativos‚îÇ
‚îÇ  ‚Ä¢ Conhecimento sem√¢ntico      ‚îÇ   ‚Ä¢ Consolida√ß√£o          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Analogia com IA**:
- **Neoc√≥rtex** = LLM (par√¢metros + corpus documental)
- **Hipocampo** = Knowledge Graph + Sistema de indexa√ß√£o

### 2.2 Tipos de Mem√≥ria de Longo Prazo

```
Mem√≥ria de Longo Prazo
‚îú‚îÄ‚îÄ Declarativa (Expl√≠cita)
‚îÇ   ‚îú‚îÄ‚îÄ Epis√≥dica ‚Üí Eventos, conversas, experi√™ncias
‚îÇ   ‚îî‚îÄ‚îÄ Sem√¢ntica ‚Üí Fatos, conceitos, conhecimento
‚îî‚îÄ‚îÄ N√£o-Declarativa (Impl√≠cita)
    ‚îú‚îÄ‚îÄ Procedural ‚Üí Habilidades, comportamentos
    ‚îú‚îÄ‚îÄ Priming ‚Üí Associa√ß√µes autom√°ticas
    ‚îî‚îÄ‚îÄ Condicionamento ‚Üí Respostas aprendidas
```

---

## 3. Arquitetura do HippocampAI

### 3.1 Componentes Principais

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    HIPPocampAI ARCHITECTURE                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ   Entrada    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Processamento‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Storage    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   (Texto)    ‚îÇ    ‚îÇ              ‚îÇ    ‚îÇ              ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ         ‚îÇ                   ‚îÇ                   ‚îÇ          ‚îÇ
‚îÇ         ‚ñº                   ‚ñº                   ‚ñº          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  OpenIE      ‚îÇ    ‚îÇ  Embeddings  ‚îÇ    ‚îÇ  Qdrant      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  (Triplas)   ‚îÇ    ‚îÇ  (Vetores)   ‚îÇ    ‚îÇ  (Vector DB) ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ         ‚îÇ                   ‚îÇ                   ‚îÇ          ‚îÇ
‚îÇ         ‚ñº                   ‚ñº                   ‚ñº          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  Knowledge   ‚îÇ    ‚îÇ  Importance  ‚îÇ    ‚îÇ  Redis       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  Graph       ‚îÇ    ‚îÇ  Scoring     ‚îÇ    ‚îÇ  (Cache)     ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                    RECUPERA√á√ÉO (Query)                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  Vector      ‚îÇ    ‚îÇ  BM25        ‚îÇ    ‚îÇ  Graph       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  Search      ‚îÇ    ‚îÇ  Keyword     ‚îÇ    ‚îÇ  Traversal   ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ         ‚îÇ                   ‚îÇ                   ‚îÇ          ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îÇ                             ‚ñº                              ‚îÇ
‚îÇ                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                        ‚îÇ
‚îÇ                    ‚îÇ  RRF Fusion  ‚îÇ                        ‚îÇ
‚îÇ                    ‚îÇ  (Reciprocal ‚îÇ                        ‚îÇ
‚îÇ                    ‚îÇ   Rank Fusion)‚îÇ                        ‚îÇ
‚îÇ                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 3.2 Pipeline de Indexa√ß√£o (Offline)

**Fase 1: Extra√ß√£o de Conhecimento**
```python
# Pseudo-c√≥digo do processo OpenIE
"""
Texto: "Jo√£o trabalha na Google como engenheiro de ML"
       ‚Üì
Triplas extra√≠das:
  - (Jo√£o, trabalha_em, Google)
  - (Jo√£o, cargo, Engenheiro de ML)
  - (Google, emprega, Jo√£o)
"""
```

**Fase 2: Enriquecimento do Grafo**
- Adicionar arestas de sinon√≠mia (baseado em embeddings)
- Calcular especificidade dos n√≥s
- Criar matriz de co-ocorr√™ncia passagem-n√≥

**Fase 3: Consolida√ß√£o (Sleep Phase)**
- Mesclar mem√≥rias relacionadas
- Decaimento de import√¢ncia temporal
- Podar mem√≥rias de baixo valor

### 3.3 Pipeline de Recupera√ß√£o (Online)

**Algoritmo: Personalized PageRank**

```python
# Pseudo-c√≥digo
"""
1. Identificar n√≥s da query no grafo
2. Ativar n√≥s iniciais com peso baseado em similaridade
3. Executar PageRank personalizado a partir desses n√≥s
4. Rankear passagens baseado na ativa√ß√£o dos n√≥s
5. Combinar com BM25 e Vector Search via RRF
"""
```

---

## 4. Tipos de Mem√≥ria

### 4.1 Mapeamento para o HippocampAI

| Tipo Biol√≥gico | Implementa√ß√£o | Uso no Chatbot |
|----------------|---------------|----------------|
| **Epis√≥dica** | `memory_type="conversation"` | Hist√≥rico de chats |
| **Sem√¢ntica** | `memory_type="fact"` | Prefer√™ncias, perfil |
| **Procedural** | `memory_type="behavioral"` | Tom de voz, estilo |
| **Working** | Context Window | Contexto imediato |

### 4.2 Estrutura de Dados

```json
{
  "memory_id": "uuid",
  "user_id": "user_123",
  "session_id": "session_456",
  "type": "episodic|semantic|procedural",
  "content": "string",
  "embedding": [0.1, 0.2, ...],
  "entities": ["entity1", "entity2"],
  "triples": [
    {"subject": "s", "predicate": "p", "object": "o"}
  ],
  "importance_score": 0.85,
  "timestamp": "2026-02-15T10:00:00Z",
  "access_count": 5,
  "last_accessed": "2026-02-15T12:00:00Z",
  "metadata": {
    "source": "conversation",
    "confidence": 0.92
  }
}
```

---

## 5. Instala√ß√£o e Configura√ß√£o

### 5.1 Pr√©-requisitos

```bash
# Infraestrutura necess√°ria
Docker 20.10+
Python 3.9+
4GB RAM m√≠nimo (8GB recomendado)
```

### 5.2 Instala√ß√£o

```bash
# M√©todo 1: Docker Compose (Recomendado)
git clone https://github.com/rexdivakar/HippocampAI.git
cd HippocampAI
docker-compose up -d

# M√©todo 2: Instala√ß√£o local (quando dispon√≠vel no PyPI)
pip install hippocampai

# M√©todo 3: Instala√ß√£o do GitHub
pip install git+https://github.com/rexdivakar/HippocampAI.git
```

### 5.3 Configura√ß√£o do Docker Compose

```yaml
# docker-compose.yml
version: '3.8'

services:
  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
    volumes:
      - qdrant_storage:/qdrant/storage

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data

  hippocampai-api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - QDRANT_URL=http://qdrant:6333
      - REDIS_URL=redis://redis:6379
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    depends_on:
      - qdrant
      - redis

volumes:
  qdrant_storage:
  redis_data:
```

### 5.4 Configura√ß√£o do Cliente

```python
# config.py
import os
from pydantic_settings import BaseSettings

class HippocampConfig(BaseSettings):
    # Vector Database
    QDRANT_URL: str = "http://localhost:6333"
    QDRANT_COLLECTION: str = "chatbot_memories"
    
    # Cache
    REDIS_URL: str = "redis://localhost:6379"
    CACHE_TTL: int = 3600  # 1 hora
    
    # LLM/Embeddings
    OPENAI_API_KEY: str = os.getenv("OPENAI_API_KEY")
    EMBEDDING_MODEL: str = "text-embedding-3-small"
    LLM_MODEL: str = "gpt-4"
    
    # Mem√≥ria
    MAX_CONTEXT_MEMORIES: int = 10
    MEMORY_DECAY_DAYS: int = 30
    IMPORTANCE_THRESHOLD: float = 0.5
    
    class Config:
        env_file = ".env"

config = HippocampConfig()
```

---

## 6. Implementa√ß√£o no Chatbot

### 6.1 Estrutura do Projeto

```
chatbot-hippocampai/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ main.py              # FastAPI app
‚îÇ   ‚îú‚îÄ‚îÄ config.py            # Configura√ß√µes
‚îÇ   ‚îú‚îÄ‚îÄ memory/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ client.py        # Cliente HippocampAI
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ types.py         # Tipos de mem√≥ria
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ manager.py       # Gerenciador de mem√≥ria
‚îÇ   ‚îú‚îÄ‚îÄ chat/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ engine.py        # Motor de conversa√ß√£o
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ context.py       # Montagem de contexto
‚îÇ   ‚îî‚îÄ‚îÄ models/
‚îÇ       ‚îî‚îÄ‚îÄ schemas.py       # Pydantic models
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ .env
```

### 6.2 Cliente HippocampAI

```python
# app/memory/client.py
from typing import List, Dict, Optional, Literal
import openai
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct
import redis
import json
import hashlib
from datetime import datetime, timedelta
import networkx as nx
from collections import defaultdict

class MemoryType:
    EPISODIC = "episodic"      # Conversas, eventos
    SEMANTIC = "semantic"      # Fatos, prefer√™ncias
    PROCEDURAL = "procedural"  # Comportamentos, estilo

class HippocampMemoryClient:
    """
    Cliente de mem√≥ria inspirado no hipocampo humano.
    Implementa Knowledge Graph + Vector Search + Cache.
    """
    
    def __init__(
        self,
        qdrant_url: str = "http://localhost:6333",
        redis_url: str = "redis://localhost:6379",
        openai_api_key: str = None,
        collection_name: str = "memories"
    ):
        self.qdrant = QdrantClient(url=qdrant_url)
        self.redis = redis.from_url(redis_url, decode_responses=True)
        openai.api_key = openai_api_key
        
        self.collection_name = collection_name
        self.embedding_model = "text-embedding-3-small"
        self.graph = nx.DiGraph()  # Knowledge Graph em mem√≥ria
        
        # Inicializar cole√ß√£o
        self._init_collection()
    
    def _init_collection(self):
        """Inicializa cole√ß√£o no Qdrant se n√£o existir"""
        try:
            self.qdrant.get_collection(self.collection_name)
        except:
            self.qdrant.create_collection(
                collection_name=self.collection_name,
                vectors_config=VectorParams(
                    size=1536,  # OpenAI embedding size
                    distance=Distance.COSINE
                )
            )
    
    def _get_embedding(self, text: str) -> List[float]:
        """Gera embedding usando OpenAI"""
        cache_key = f"emb:{hashlib.md5(text.encode()).hexdigest()}"
        cached = self.redis.get(cache_key)
        
        if cached:
            return json.loads(cached)
        
        response = openai.embeddings.create(
            input=text,
            model=self.embedding_model
        )
        embedding = response.data[0].embedding
        
        # Cache por 24 horas
        self.redis.setex(cache_key, 86400, json.dumps(embedding))
        return embedding
    
    def _extract_triples(self, text: str) -> List[Dict]:
        """
        Extrai triplas (sujeito, predicado, objeto) do texto.
        Simplifica√ß√£o - em produ√ß√£o usar OpenIE ou LLM.
        """
        # TODO: Implementar extra√ß√£o real usando LLM
        # Exemplo simplificado:
        triples = []
        # L√≥gica de extra√ß√£o aqui
        return triples
    
    def _calculate_importance(self, text: str, memory_type: str) -> float:
        """
        Calcula score de import√¢ncia baseado em:
        - Tipo de mem√≥ria
        - Entidades nomeadas
        - Sentimento
        - Urg√™ncia impl√≠cita
        """
        base_score = 0.5
        
        # Mem√≥rias procedurais t√™m maior import√¢ncia
        if memory_type == MemoryType.PROCEDURAL:
            base_score += 0.3
        
        # Mem√≥rias com entidades espec√≠ficas s√£o mais importantes
        # TODO: An√°lise mais sofisticada
        
        return min(base_score, 1.0)
    
    def remember(
        self,
        content: str,
        user_id: str,
        memory_type: Literal["episodic", "semantic", "procedural"] = "episodic",
        session_id: Optional[str] = None,
        metadata: Optional[Dict] = None
    ) -> str:
        """
        Armazena uma nova mem√≥ria.
        
        Args:
            content: Conte√∫do da mem√≥ria
            user_id: ID do usu√°rio
            memory_type: Tipo de mem√≥ria
            session_id: ID da sess√£o (opcional)
            metadata: Metadados adicionais
            
        Returns:
            memory_id: ID √∫nico da mem√≥ria
        """
        # Gerar embedding
        embedding = self._get_embedding(content)
        
        # Extrair entidades e triplas
        triples = self._extract_triples(content)
        entities = list(set([t["subject"] for t in triples] + 
                           [t["object"] for t in triples]))
        
        # Calcular import√¢ncia
        importance = self._calculate_importance(content, memory_type)
        
        # Criar ponto de mem√≥ria
        memory_id = hashlib.md5(
            f"{user_id}:{content}:{datetime.now()}".encode()
        ).hexdigest()
        
        memory_data = {
            "id": memory_id,
            "user_id": user_id,
            "session_id": session_id,
            "type": memory_type,
            "content": content,
            "entities": entities,
            "triples": triples,
            "importance_score": importance,
            "timestamp": datetime.now().isoformat(),
            "access_count": 0,
            "metadata": metadata or {}
        }
        
        # Armazenar no Qdrant
        self.qdrant.upsert(
            collection_name=self.collection_name,
            points=[PointStruct(
                id=memory_id,
                vector=embedding,
                payload=memory_data
            )]
        )
        
        # Atualizar Knowledge Graph
        for triple in triples:
            self.graph.add_edge(
                triple["subject"],
                triple["object"],
                relation=triple["predicate"],
                memory_id=memory_id
            )
        
        # Invalidar cache de consultas relacionadas
        self._invalidate_user_cache(user_id)
        
        return memory_id
    
    def recall(
        self,
        query: str,
        user_id: str,
        top_k: int = 5,
        memory_type: Optional[str] = None,
        use_graph: bool = True
    ) -> List[Dict]:
        """
        Recupera mem√≥rias relevantes usando m√∫ltiplas estrat√©gias.
        
        Estrat√©gias:
        1. Vector Search (similaridade sem√¢ntica)
        2. BM25 (matching lexical)
        3. Graph Traversal (se use_graph=True)
        4. RRF Fusion (combina√ß√£o dos rankings)
        """
        # Check cache
        cache_key = f"recall:{user_id}:{hashlib.md5(query.encode()).hexdigest()}"
        cached = self.redis.get(cache_key)
        if cached:
            return json.loads(cached)
        
        # 1. Vector Search
        query_embedding = self._get_embedding(query)
        vector_results = self.qdrant.search(
            collection_name=self.collection_name,
            query_vector=query_embedding,
            query_filter={
                "must": [{"key": "user_id", "match": {"value": user_id}}]
            },
            limit=top_k * 2
        )
        
        memories = []
        vector_scores = {}
        
        for idx, result in enumerate(vector_results):
            memory = result.payload
            if memory_type and memory["type"] != memory_type:
                continue
            
            memory["vector_score"] = result.score
            vector_scores[memory["id"]] = idx + 1
            memories.append(memory)
        
        # 2. Graph-based retrieval (Personalized PageRank simplificado)
        if use_graph and self.graph.number_of_nodes() > 0:
            # Encontrar n√≥s relevantes na query
            query_entities = self._extract_entities_from_query(query)
            
            if query_entities:
                # Calcular PageRank a partir dos n√≥s da query
                pagerank = nx.pagerank(
                    self.graph,
                    personalization={
                        node: 1.0 for node in query_entities 
                        if node in self.graph
                    }
                )
                
                # Boostar mem√≥rias conectadas aos n√≥s importantes
                for memory in memories:
                    graph_score = 0
                    for entity in memory.get("entities", []):
                        if entity in pagerank:
                            graph_score += pagerank[entity]
                    memory["graph_score"] = graph_score
        
        # 3. RRF (Reciprocal Rank Fusion)
        final_scores = {}
        
        for memory in memories:
            mid = memory["id"]
            
            # Vector rank
            vector_rank = vector_scores.get(mid, 1000)
            
            # Importance boost
            importance_boost = memory.get("importance_score", 0.5) * 10
            
            # Recency boost
            days_old = (datetime.now() - datetime.fromisoformat(
                memory["timestamp"]
            )).days
            recency_boost = max(0, 30 - days_old) / 3
            
            # RRF Score
            final_scores[mid] = (
                1.0 / (60 + vector_rank) +  # Vector
                1.0 / (60 + importance_boost) +  # Importance
                1.0 / (60 + recency_boost)       # Recency
            )
            
            if "graph_score" in memory:
                final_scores[mid] += memory["graph_score"] * 0.1
        
        # Ordenar e retornar top_k
        sorted_memories = sorted(
            memories,
            key=lambda x: final_scores.get(x["id"], 0),
            reverse=True
        )[:top_k]
        
        # Atualizar access_count
        for memory in sorted_memories:
            self._update_access_stats(memory["id"])
        
        # Cache resultados por 5 minutos
        self.redis.setex(cache_key, 300, json.dumps(sorted_memories))
        
        return sorted_memories
    
    def _extract_entities_from_query(self, query: str) -> List[str]:
        """Extrai entidades da query do usu√°rio"""
        # TODO: Implementar NER
        return []
    
    def _update_access_stats(self, memory_id: str):
        """Atualiza estat√≠sticas de acesso"""
        # TODO: Implementar atualiza√ß√£o no Qdrant
        pass
    
    def _invalidate_user_cache(self, user_id: str):
        """Invalida cache do usu√°rio quando novas mem√≥rias s√£o adicionadas"""
        pattern = f"recall:{user_id}:*"
        for key in self.redis.scan_iter(match=pattern):
            self.redis.delete(key)
    
    def consolidate(self, user_id: str):
        """
        Fase de sono: consolida mem√≥rias relacionadas.
        - Mescla mem√≥rias similares
        - Remove duplicatas
        - Atualiza import√¢ncia baseado em acessos
        """
        # TODO: Implementar consolida√ß√£o
        pass
    
    def get_memory_context(
        self,
        user_id: str,
        current_query: str,
        max_tokens: int = 2000
    ) -> str:
        """
        Monta contexto de mem√≥ria para o LLM.
        
        Estrat√©gia:
        1. Recuperar mem√≥rias relevantes
        2. Ordenar por relev√¢ncia e import√¢ncia
        3. Respeitar limite de tokens
        4. Formatar para o prompt
        """
        memories = self.recall(current_query, user_id, top_k=10)
        
        if not memories:
            return ""
        
        context_parts = []
        current_tokens = 0
        
        # Mem√≥rias sem√¢nticas primeiro (perfil, prefer√™ncias)
        semantic = [m for m in memories if m["type"] == MemoryType.SEMANTIC]
        episodic = [m for m in memories if m["type"] == MemoryType.EPISODIC]
        procedural = [m for m in memories if m["type"] == MemoryType.PROCEDURAL]
        
        # Ordem de prioridade: Procedural > Sem√¢ntico > Epis√≥dico
        ordered = procedural + semantic + episodic
        
        for memory in ordered:
            content = memory["content"]
            mem_type = memory["type"]
            
            # Estimativa simples de tokens (1 token ‚âà 4 chars)
            estimated_tokens = len(content) // 4 + 10
            
            if current_tokens + estimated_tokens > max_tokens:
                break
            
            prefix = {
                MemoryType.SEMANTIC: "[Fato]",
                MemoryType.EPISODIC: "[Hist√≥rico]",
                MemoryType.PROCEDURAL: "[Estilo]"
            }.get(mem_type, "[Info]")
            
            context_parts.append(f"{prefix} {content}")
            current_tokens += estimated_tokens
        
        return "\\n".join(context_parts)
```

### 6.3 Motor de Chat com Mem√≥ria

```python
# app/chat/engine.py
from typing import List, Dict
import openai
from app.memory.client import HippocampMemoryClient, MemoryType

class HippocampChatEngine:
    """
    Motor de conversa√ß√£o com mem√≥ria de longo prazo.
    """
    
    def __init__(self, memory_client: HippocampMemoryClient):
        self.memory = memory_client
        self.conversation_buffer = {}  # Buffer por sess√£o
    
    async def chat(
        self,
        message: str,
        user_id: str,
        session_id: str,
        system_prompt: str = None
    ) -> Dict:
        """
        Processa mensagem do usu√°rio com contexto de mem√≥ria.
        """
        # 1. Recuperar contexto de mem√≥ria
        memory_context = self.memory.get_memory_context(
            user_id=user_id,
            current_query=message,
            max_tokens=1500
        )
        
        # 2. Construir mensagens para o LLM
        messages = self._build_messages(
            system_prompt=system_prompt,
            memory_context=memory_context,
            user_message=message,
            session_id=session_id
        )
        
        # 3. Gerar resposta
        response = await openai.chat.completions.create(
            model="gpt-4",
            messages=messages,
            temperature=0.7,
            max_tokens=1000
        )
        
        assistant_message = response.choices[0].message.content
        
        # 4. Armazenar intera√ß√£o na mem√≥ria
        self._store_interaction(
            user_id=user_id,
            session_id=session_id,
            user_message=message,
            assistant_message=assistant_message
        )
        
        # 5. Extrair e armazenar fatos importantes
        await self._extract_facts(
            user_id=user_id,
            session_id=session_id,
            conversation=[message, assistant_message]
        )
        
        return {
            "response": assistant_message,
            "memories_used": len(memory_context.split("\\n")) if memory_context else 0,
            "session_id": session_id
        }
    
    def _build_messages(
        self,
        system_prompt: str,
        memory_context: str,
        user_message: str,
        session_id: str
    ) -> List[Dict]:
        """Constr√≥i lista de mensagens para o LLM"""
        
        # System prompt com contexto de mem√≥ria
        full_system = system_prompt or "Voc√™ √© um assistente √∫til."
        
        if memory_context:
            full_system += f"\\n\\nContexto do usu√°rio:\\n{memory_context}"
        
        messages = [
            {"role": "system", "content": full_system},
        ]
        
        # Adicionar hist√≥rico recente do buffer
        if session_id in self.conversation_buffer:
            messages.extend(self.conversation_buffer[session_id][-6:])  # √öltimas 3 intera√ß√µes
        
        # Mensagem atual
        messages.append({"role": "user", "content": user_message})
        
        return messages
    
    def _store_interaction(
        self,
        user_id: str,
        session_id: str,
        user_message: str,
        assistant_message: str
    ):
        """Armazena intera√ß√£o na mem√≥ria epis√≥dica"""
        
        # Armazenar mensagem do usu√°rio
        self.memory.remember(
            content=f"Usu√°rio disse: {user_message}",
            user_id=user_id,
            memory_type=MemoryType.EPISODIC,
            session_id=session_id,
            metadata={"role": "user", "session_id": session_id}
        )
        
        # Armazenar resposta do assistente
        self.memory.remember(
            content=f"Assistente respondeu: {assistant_message}",
            user_id=user_id,
            memory_type=MemoryType.EPISODIC,
            session_id=session_id,
            metadata={"role": "assistant", "session_id": session_id}
        )
        
        # Atualizar buffer
        if session_id not in self.conversation_buffer:
            self.conversation_buffer[session_id] = []
        
        self.conversation_buffer[session_id].extend([
            {"role": "user", "content": user_message},
            {"role": "assistant", "content": assistant_message}
        ])
        
        # Manter apenas √∫ltimas 10 intera√ß√µes no buffer
        self.conversation_buffer[session_id] = self.conversation_buffer[session_id][-20:]
    
    async def _extract_facts(
        self,
        user_id: str,
        session_id: str,
        conversation: List[str]
    ):
        """
        Usa LLM para extrair fatos sem√¢nticos da conversa.
        """
        prompt = f"""Analise a seguinte conversa e extraia fatos importantes sobre o usu√°rio 
(prefer√™ncias, informa√ß√µes pessoais, objetivos). Retorne como bullet points:

Conversa:
{chr(10).join(conversation)}

Fatos:"""
        
        response = await openai.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
            max_tokens=200
        )
        
        facts_text = response.choices[0].message.content
        
        # Armazenar cada fato como mem√≥ria sem√¢ntica
        for line in facts_text.split("\\n"):
            line = line.strip()
            if line and line.startswith("-"):
                fact = line[1:].strip()
                self.memory.remember(
                    content=fact,
                    user_id=user_id,
                    memory_type=MemoryType.SEMANTIC,
                    session_id=session_id,
                    metadata={"extracted": True, "source": "conversation"}
                )
```

### 6.4 API FastAPI

```python
# app/main.py
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import Optional, List
import uuid

from app.memory.client import HippocampMemoryClient
from app.chat.engine import HippocampChatEngine
from app.config import config

app = FastAPI(title="Chatbot com HippocampAI")

# Inicializar clientes
memory_client = HippocampMemoryClient(
    qdrant_url=config.QDRANT_URL,
    redis_url=config.REDIS_URL,
    openai_api_key=config.OPENAI_API_KEY
)

chat_engine = HippocampChatEngine(memory_client)

# Models
class ChatRequest(BaseModel):
    message: str
    user_id: str
    session_id: Optional[str] = None
    system_prompt: Optional[str] = None

class ChatResponse(BaseModel):
    response: str
    session_id: str
    memories_used: int

class MemoryRequest(BaseModel):
    content: str
    user_id: str
    memory_type: str = "episodic"
    metadata: Optional[dict] = None

@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """Endpoint principal de conversa√ß√£o"""
    try:
        session_id = request.session_id or str(uuid.uuid4())
        
        result = await chat_engine.chat(
            message=request.message,
            user_id=request.user_id,
            session_id=session_id,
            system_prompt=request.system_prompt
        )
        
        return ChatResponse(
            response=result["response"],
            session_id=result["session_id"],
            memories_used=result["memories_used"]
        )
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/memory")
async def create_memory(request: MemoryRequest):
    """Adicionar mem√≥ria manualmente"""
    try:
        memory_id = memory_client.remember(
            content=request.content,
            user_id=request.user_id,
            memory_type=request.memory_type,
            metadata=request.metadata
        )
        return {"memory_id": memory_id, "status": "created"}
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/memory/{user_id}")
async def get_memories(
    user_id: str,
    query: Optional[str] = None,
    memory_type: Optional[str] = None,
    limit: int = 10
):
    """Recuperar mem√≥rias do usu√°rio"""
    try:
        memories = memory_client.recall(
            query=query or "*",
            user_id=user_id,
            top_k=limit,
            memory_type=memory_type
        )
        return {"memories": memories, "count": len(memories)}
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/consolidate/{user_id}")
async def consolidate_memories(user_id: str):
    """Executar consolida√ß√£o de mem√≥rias (fase de sono)"""
    try:
        memory_client.consolidate(user_id)
        return {"status": "consolidation_complete", "user_id": user_id}
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

---

## 7. Casos de Uso

### 7.1 Assistente Pessoal

```python
# Exemplo: Assistente que lembra prefer√™ncias

async def personal_assistant_example():
    """
    Demonstra√ß√£o de mem√≥ria sem√¢ntica para prefer√™ncias.
    """
    user_id = "user_123"
    
    # Conversa 1 - Primeira intera√ß√£o
    result1 = await chat_engine.chat(
        message="Meu nome √© Carlos e sou al√©rgico a amendoim",
        user_id=user_id,
        session_id="session_1"
    )
    
    # Conversa 2 - Dias depois (nova sess√£o)
    result2 = await chat_engine.chat(
        message="Quero pedir comida, o que voc√™ recomenda?",
        user_id=user_id,
        session_id="session_2"  # Nova sess√£o!
    )
    
    # O assistente deve lembrar da alergia e sugerir op√ß√µes seguras
    print(result2["response"])
    # Sa√≠da esperada: "Ol√° Carlos! Considerando sua alergia a amendoim, 
    # recomendo..."
```
