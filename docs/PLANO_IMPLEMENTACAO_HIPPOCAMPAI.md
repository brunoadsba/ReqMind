# üß† Plano de Implementa√ß√£o: HippocampAI (Vers√£o Lite/KISS)

Este plano descreve como implementar o sistema de mem√≥ria **HippocampAI** no projeto, respeitando as restri√ß√µes de arquitetura (KISS, Features-based) e infraestrutura (Docker simples).

## üéØ Objetivo
Transformar o bot em um agente com **mem√≥ria de longo prazo real**, capaz de lembrar fatos, prefer√™ncias e hist√≥rico complexo, usando uma arquitetura h√≠brida (Vetores + Grafo) por√©m leve (sem novos containers pesados).

---

## üõ†Ô∏è Stack Tecnol√≥gica (Lite Version)
Para manter o princ√≠pio **KISS** e evitar complexidade de infraestrutura (Qdrant/Redis exigem muita RAM), usaremos:

1.  **Vector Store:** `ChromaDB` (Embarcado)
    *   *Por que?* Roda no mesmo processo Python, persist√™ncia em arquivo, sem necessidade de container extra.
2.  **Graph Store:** `NetworkX` + `JSON/Pickle`
    *   *Por que?* Suficiente para grafos de conhecimento pessoais (< 10k n√≥s).
3.  **Cache:** `LRU Cache` (In-memory) + `SQLite`
    *   *Por que?* Redis √© overkill para um √∫nico usu√°rio.
4.  **LLM:** `Groq` (Llama 3.3/4)
    *   *Por que?* J√° integrado e r√°pido.

---

## üìÖ Roteiro de Implementa√ß√£o

### Fase 1: Estrutura & Depend√™ncias (Dia 1)
O foco √© preparar o terreno sem quebrar o bot atual.

1.  **Criar Feature Module:**
    *   `src/features/hippocampus/` (Respeitando regra: "Todo c√≥digo novo em src/features/")
    *   `src/features/hippocampus/client.py` (L√≥gica principal)
    *   `src/features/hippocampus/graph.py` (Gerenciador do Grafo)
    *   `src/features/hippocampus/types.py` (Data models)

2.  **Atualizar Depend√™ncias:**
    *   Adicionar `chromadb`, `networkx` ao `requirements.txt`.
    *   Rebuild do Docker.

### Fase 2: O Motor Hippocampus (Dia 2)
Implementar a l√≥gica de armazenamento e recupera√ß√£o h√≠brida.

1.  **Implementar `HippocampusClient`:**
    *   **Ingest√£o (`remember`):**
        *   Recebe texto -> Gera Embedding (via OpenAI ou Llama/Groq se poss√≠vel, ou `sentence-transformers` local).
        *   Extrai Triplas (Sujeito-Verbo-Objeto) via LLM.
        *   Salva Vetor no ChromaDB.
        *   Atualiza Grafo NetworkX.
    *   **Recupera√ß√£o (`recall`):**
        *   Busca Vetorial (ChromaDB).
        *   Busca no Grafo (PageRank simplificado).
        *   Reranking (RRF Fusion).

2.  **Camada de Persist√™ncia:**
    *   Garantir que o Grafo seja salvo em `dados/hippocampus_graph.json` a cada atualiza√ß√£o.
    *   ChromaDB persistindo em `dados/chroma_db`.

### Fase 3: Integra√ß√£o com o Agente (Dia 3)
Conectar o "c√©rebro" ao "corpo" do bot.

1.  **Modificar `src/bot_simple.py`:**
    *   Inicializar `HippocampusClient` no startup.
    *   Injetar cliente no contexto do `Agent`.

2.  **Hook de Mem√≥ria no `Agent`:**
    *   **Antes de responder:** `hippocampus.recall(query)` -> Adicionar contexto ao System Prompt.
    *   **Depois de responder:** `hippocampus.remember(interaction)` (Async).

3.  **Extra√ß√£o de Fatos em Background:**
    *   Criar task que roda ap√≥s cada conversa para extrair fatos importantes ("O usu√°rio disse que gosta de X") e salvar como Mem√≥ria Sem√¢ntica.

### Fase 4: Consolida√ß√£o ("Sleep Phase") (Dia 4)
Manuten√ß√£o da mem√≥ria.

1.  **Script de Consolida√ß√£o (`scripts/consolidate_memory.py`):**
    *   L√™ mem√≥rias recentes.
    *   Remove redund√¢ncias.
    *   Aplica decaimento temporal (esquece detalhes irrelevantes).
    *   Executar via Cron ou comando `/consolidar`.

---

## üìÇ Estrutura de Diret√≥rios Proposta

```bash
src/
  features/
    hippocampus/
      __init__.py
      client.py       # Fachada principal
      vector_store.py # Wrapper ChromaDB
      graph_store.py  # Wrapper NetworkX
      models.py       # Pydantic models (Memory, Triple)
      prompts.py      # Prompts para extra√ß√£o de fatos/triplas
```

## ‚ö†Ô∏è Pontos de Aten√ß√£o
*   **Embeddings:** Usar `sentence-transformers` (local, CPU) pode ser lento no Docker se n√£o tiver cuidado. Alternativa: Usar API da OpenAI (paga) ou Groq (se suportar embeddings, ainda limitado). *Recomenda√ß√£o: `all-MiniLM-L6-v2` (r√°pido e leve).*
*   **Tokens:** Extrair triplas consome tokens do LLM. Usar modelos menores (Llama 3.3 8B ou at√© Gemma 2B local se poss√≠vel) para essa tarefa auxiliar.

## üöÄ Pr√≥ximos Passos
1.  Aprovar este plano.
2.  Criar branch `feature/hippocampus`.
3.  Instalar depend√™ncias.
